[
    {
        "transcript": "As you guys have probably already noticed that I'm currently an assistant professor at our AI program.",
        "week": 1,
        "page": 1
    },
    {
        "transcript": "And let me quickly talk about my research. So, here is my email address, if you have any questions regarding to your course or regarding to your future, you know, or any other minor stuff you can feel free send me an email. My research and regular focus are on these 4 directions. So first of all, I'll majority of my papers, I focus on transfer learning. So specifically, I focused on as more domain for the domain adaptation. So later I will explain what the domain adaptation meaning here. Next, I am interested in manifold learning, which is kind of an expansion of space is different from our training space. And next, I am interested in the shape analysis, and this is really, relatively related to the medical field. And lastly, recently, I'm more focused on the audio denoising problem.",
        "week": 1,
        "page": 2
    }
]
// -------------------------------------------------------------------------------------------------------

[
    {
        "transcript": "So you have a process, and several outcomes are possible. When the process is repeated a large number of times, each outcome occurs with a relative frequency, or so-called probability.    If a particular outcome occurs more often, we say it is more probable. So, the probability arises through a contest.First of all, in an actual repeated experiment (for example, recalling the color of 1,000 cars driving by), if 57 out of 1,000 are green, then you estimate the probability of a car being green as 57 divided by 1,000. Eventually, you get a small number, and this is the so-called probability here.In the idealized conceptions of a repeated process, for example, considering the behavior of an unbiased 6-sided die, the probability of rolling a 5 is 1 over 6.That should be the same. Another example is when you need a model for how people's heights are distributed. You choose a normal distribution to represent the expected radio probabilities",
        "week": 2,
        "page": 2
    },
    {
        "transcript": "So next, when trying to solve machine learning problems, you may need to deal with some uncertainty and qualities, as well as the stochastic problem in non-deterministic parties.       In probability theory, it provides a mathematical framework for representing and quantifying uncertain quantities, and there are several different sources of uncertainties.For example, the first one can be the inherent stochasticity in the system being modeled. Another concrete example is that most interpretations of quantum mechanisms describe the dynamics of some atom particles as being probabilistic.Another reason causing uncertainty can be the incompleteness of observability. Even deterministic systems can appear stochastic when we cannot observe all the variables that affect the behavior of the system. Another reason can be incomplete modeling. When we use models, we may not capture all the information we observe, leading to uncertainty in the model's predictions. For example, the discretization of real number values, dimension reduction, etc. Because we always discard unnecessary information. So, while it may seem unnecessary, in terms of data itself, it can be useful.",
        "week": 3,
        "page": 3
    },
    {
        "transcript": "That was another reason for the uncertainty of the model. Now, let's quickly review random variables. A random variable, denoted as x, is a variable that can take different values.For example, x can represent drawing time. The possible values of x comprise a sample space. For instance, consider the outcome space, denoted as S, which is equal to 1 to 6 because it represents a six-sided die with six different values.So, we denote the event of rolling a 5 as x equal to 5 or x being equal to 5 with this equation.The probability is denoted as P, and it is used to express the likelihood of events. Additionally, the notation P(x) can be used to denote the probability distribution of the random variable x. It signifies that x has a probability of P(x). For a random variable, there can be both discrete and continuous types. Discrete random variables have a finite number of possible values. For example, the sides of a die represent a finite number of states. On the other hand, continuous random variables can have an infinite number of possible states. For instance, the height of a person can vary infinitely",
        "week": 3,
        "page": 4
    },
    {
        "transcript": "Here are some, like, kind of rules of the probability. For example, the probability of an event A. Yeah. Given sample space test, the net as the P(A) master satisfies the following properties. Like, that is a non negativity, so which means that probability of all event, it should be bigger than 0. And all possible outcomes is, For example, if we sum all the, probability, then it should be equal to 1. And, yeah, another property is about Identity of the this joint event, which means that, for example,  A1  and the A2 used to be equal to 0. So which means that there is no, common unit between this A1 and A2. And also the Probability of A1, the unit was the h two should be equal to the probability of A1 then cross all probability of A2. And next, the probability of a random variable k x must obey the exons of the probability over the possible values in the sample space S",
        "week": 3,
        "page": 5
    },
    {
        "transcript": "And here's another example of our discretes of variables.A probability distribution of discrete variables may be, described as a probability mass function. So also named the PMF. So let me you might need to remember what is PMF. That is just probability mass function as you can see here. And our probability distribution over continuous variable may be described as an PDI, because of probability density function. For example, waiting time between the, eruption of all the, best film and the PDI gives the probability. And even, if not just keep small region with the random of the that acts here to find a probability of the interval a and b where you already have of the integration between of a and b of the probability p(x) and with p(x). Because this is how we, find this probability over kind of certain interval there.",
        "week": 3,
        "page": 6
    },
    {
        "transcript": "For the market variable of random variables, so we may need to consider several random variables at one time and then if several random process that occurred in parallel or in sequence, for example, to model that relationship is several diseases and the symptoms. And, another example is if you process images with millions of pixels,  then each pixel is a one random variable you can see, in this way. That's the way we are studying the probability distribution defined over the multiple random variables, And those include the joint, the condition, and marginal distribution. The individual random variables can be grouped together into a random vector because they represent different properties of an individual's, status to go unit. Our market variant of random variable is a vector of multiple random variables. So for example, I the capital x can, be represented by  X1, X2 to Xn with its transpose state.",
        "week": 3,
        "page": 7
    },
    {
        "transcript": "So now let's talk about 1st about the joint, probability distribution. So the probability distribution that acts many variables at that time is known as a joint probability distribution to arrive, but you're able to see about all That is capital x is equal to s, small s, and, capital y is equal to small y simultaneously. So that is Probability of x equal to smart x and the capital y would equal to small y, the less is a joint probability. And, women Also, right, it is a p(x) for the initial to represent this probability. And from this Figure, you have found that as I join the probability, p one x is equal to mini event. So the mini event is here. Right? This is a mini event. And why why is European? So you will get us a joint of probability. If tax is equal to a minimum Why is equal European? Then we cannot get it on probability of 0.1481. So this is a we need to consider about the post case.",
        "week": 3,
        "page": 8
    },
    {
        "transcript": "And our distribution is called the marginal probability distribution. So for the marginal probability distribution is a problem in a probability distribution of a single variable. So previous, but jointly, consider 2. Right? But here, we only consider about Europe and just consider about 1. It'll calculate the best time that joined them probability distribution to just push P (X,Y). For example, we may need to use, some rule. For example, the probability of the capital x is equal to small x. So there should be a we need to sum all the probability across other kind of variable than the y. So as you can see here, either some or or other probabilities across the y here. Right? And for continuous random variable, some measure is replaced by indication. So here, just the indication up here. And this process is called, marginalization. So as you can see, this figure So now let's say, what's the probability of the, marginal probability of the p x equal to minivan. So That basically we need to calculate as our probability of the by x is equal to million, then try to sum All these 3 brought together. So eventually, we can get a 0. 3333 as it's minor distribution of x equal to any vector.",
        "week": 3,
        "page": 9
    },
    {
        "transcript": "The last, distribution we want to mention is about the conditional probability distribution. As you can see from the name, so the conditional probability distribution is a probability distribution of 1 variable provides that another variable has a taken, certain value, for example, it can be donated in this query and solve probability of x is dependent on given the small y here. And, also, it can be right in this equation. And, also, similarly, in this figure, if we want, you can calculate the conditionally probability of y is equal to Europeans and x is equal to a minivan, so we can Calculators, probability. First of all, we need to find his joint view. Probability that is about this one. Right? Then divide by the all other case. So eventually, we can get this this kind of probability of y is equal to, European giving x is equal to many events. Now can I get a 0. 4 or so 3 as is contingent conditional probability.",
        "week": 3,
        "page": 10
    },
    {
        "transcript": "Okay. I think this is a nice device. The for, you know, is that is Bayesian equation. Right? So our calculated that conditional probability for one variable, but additional probability support another variable unknown. For example, this is a probability of x given y that is And we take ready in this secret. It's called a base rule. And, again, it's the multiplication rule of that for the joint distribution is used. As you can see it's just I cannot move a little bit to the last side. Bayes symmetry, we have another, p y of x that is equal problem. Can you show probability of a x given y that times the probability of p y? And there's a term I'll refer as the p(x) here, just, we can also name it as a prior probability or the initial degree of of x. And he has, p x y is a posterior probability. That degree of We we live after the incorporation of knowledge of y and the p y x is so called the likelihood of the y given x, and the p y here is just the evidence. And, eventually, the is just, can be understand in this way. So later, once we Try to estimate something we were trying to reuse this likelihood. It was kind of prior",
        "week": 3,
        "page": 11
    },
    {
        "transcript": "And, that will talk about the independence. So the 2 random variable x And the y are independent if it was a occurrence of y does not reveal any information about the occurrence of x. For example, true success, true success of the rules of the die, independent. Right? So therefore, we have y, the p (x | y) that is just equal to p(x) because they are independent. And as you guys have notation here, as notated from Independent random variables that p(x,y), that is just equal to the times up to p(x) times with p(y). In other cases, the random variables are dependent. For example, getting a king, our successive draws from our deck, the draw card is not a replacement. And the 2 random variables, x, y, are conditionally independent to give another random variable z if and only if them p x y given z, that is equal to p(x) given z times of is, the probability of y given z here and also people make it clear.",
        "week": 3,
        "page": 12
    },
    {
        "transcript": "Among the, continuous multivariate just means that for some concept of joint, marginal, and the conditional probabilities apply for continuous random variables. So the probability distributions use the indication of continuous random variables instead of us. Summation of a dissipated random variables. Here is an example about the 3 component of gaussian mixture from the distribution in 3 d dimension.",
        "week": 3,
        "page": 13
    },
    {
        "transcript": "The expected value. So the expected value or the expectation of a function f(x), Yeah. With respect to to probability distribution p x as, average or the means of the mean, y x is drawn from p(x). For example, for our discrete random variable x, it is can be calculated using pretty best way. We And the estimation of all the p(x) probability of x that time is a bunch of x that had a sum of it. However, usually, it is our continuous random variable facts that we are being calculated using some hidden indication here. Right? So when the identity of the distribution is clear around the context, we can write, you know, as this kind of explanation here. So if it is clear with random variable, it's used, and we can write just write it as the expectation of. So b is the most common matter of central tendency of a distribution for a random variable f(x) So I is equal to f(x). That means that mu is equal to the expectation of x I. Then can be plus or rewrite at this point. As this signal, I just missed that you get well, categorize the average of all the samples xi. Right? Other measures of central tendency, Like, for example, like a medium or a techno mode of the data.",
        "week": 3,
        "page": 14
    },
    {
        "transcript": "What about the various? Varies gives the measure of how much the values of function f x, divide from Give it from the expected values as we sample values of x from p(x). And then here is the clear on how we calculate the variance here. Once the virus is low, the value of f(x) faster near the expected value. And, the you already the variance is the commonly the nature they use is sigma squared here, and then the above, we create a similar to a bunch of x. F(x) is equal to xi minus mu here. Expand. Mu is Just the main line here. That is a cosmine line, this one. And we have, like, sigma square can be rewriting this equation, and this is similar to the formula of calculating the variance of sample of observation. So as you guys might be familiar with this So I write out particular various, you know, parts of the data. And here is how we define the the standard deviation. That's just a single here. The the best theorem as per root of the various x here.",
        "week": 3,
        "page": 15
    },
    {
        "transcript": "The covariance gives the merit of how much 2 random variables are linear related to each other.And then here is the is the gradient. And, let's say, if the f xi is equal to f xi minus mu x, and the g at yi is equal to yi minus the mu y here. And then the covariance of x y can be donated in this way and then compared to where covariance of actual samples, so you can calculate it in this way. We needed to man notice that the covariance measures, the tenders of x y to divide from the, miss in the same or opposite direction at the same time. For example, Is this one kind of observe some covariance between x y? No. Right? But go to Is that right? How about this one? You probably can find a high correlation between x and y. And this should best miss a positive one. And if you like, this way should be on that relationship with an x and y. Any questions? No.",
        "week": 3,
        "page": 16
    },
    {
        "transcript": "Sure. The correlation So correlation coefficient is a covariance, normalization by the standard deviation of the 2 variables that is just And the k rate device, covariance of x y is then divided by the Sigma of x times the Sigma of y. And I also named the Pearson's correlation or or efficient at the end as, row x y here. And there's a value This one is what? I'm running the relation, but some but you can read something that I want variable is decreased. So, your rate was negative. Yeah. And the y adjustment is positive. And then it only reflects the linear dependency between variables, and it does not measure the nonlinear dependencies between the variable. So for example, in this case so So here, it's about linear dependency with noise.       From one tip place, very the higher positive than 0. 8, 0. 4, The 0, we are assuming there's no there's no kind of, correlation and, -0.4, 0. 4 and -0.8, then to -1 was means a strong negative. And, also, here is the linear dependence without noise. It basically means that you can find a very clear relationship between each other. So what about nonlinear? That's about nonlinear.",
        "week": 3,
        "page": 17
    },
    {
        "transcript": "A covariance matrix consists of multi variant of band random variable x with states of x that is, n dimensional real values has a m by m metric, such as the covariance xij that is equal to the covariance of xi and the yi. It's about it's the overall metrics. And there's a diagonal element of the covariant metrics, the various of the elements of the vector, so the the covariance of xi and, xi should be just equal to the variance of xi. So, also, we as noted that the program metrics is the metric since the program is of x I, xj that is equal to the rest of xj and, xi here.",
        "week": 3,
        "page": 18
    },
    {
        "transcript": "Was a different, probability distributions. So for example, about Bernollie distribution, it is like a binary random variable x instead of 0 and 1.  So the random variable can encoders, co inflate with the common comes up 1 with a probability of b and, 0 place probability of 1 minus p. Right? Notation should be x that is, should be should be compiled with the of p here, like your. That's what a uniform distribution, the probability of each value. For example, 'i', in the one to the n is, the is equal to the pi is equal to 1 over n. So this is notation. It should be x. It should be  I could tell you it's you and here. So what about in the figure here is equal to 5, and the probability is just 0.2. Right? Why? Why the point is is 0.2? Yeah. I'm gonna do this more like this system. And it's 8 with the files. Each property is just 0.5. Yeah. 0.2. ",
        "week": 3,
        "page": 19
    },
    {
        "transcript": "We also have other distributions. For example, Binomial distribution to perform a sequence of an independent experiments, each of which has a probability of P of succeeding. For example, we are the p should be between 0 and 1. So the property of getting case successful in the end trials should be like Donate as is the and this notation should be x, the, binomial, and p here. And we also have poison distribution, which means that a number of event, operate independently in a fixed interval of time with a known random lambda here. Just create a random variable x with a state of care that is it 1 to to n as a probability of this So he has a red, the down times the average number of occurrence of event, and then you can relate as this.",
        "week": 3,
        "page": 20
    },
    {
        "transcript": "And this should be most of the famous distribution that is Gaussian distribution. So the most well known noise distribution, you refer to the normal distribution or the informally of the pair shape distribution, and the pair is the mean and its variance and its notation. This is this is the full equation I've got, And then you guys should have familiar with our different and I often current distribution here.",
        "week": 3,
        "page": 21
    },
    {
        "transcript": "For the multinomial distribution, it is an extension of the Bernoulli distribution for binary class of body class. Remember, previously mentioned about learning is just a binary. Right? Like a frame or a coin, just to have two sides there. Distribution is called the category code distribution or generalized distribution. Is probability distribution that describes the possible results of, random variables that can take in, one of k possible categories. So a category of random variable is a discrete random variable with more than 2 possible outcomes such as the role of value. As a, for example, in a monkey class classification in machine learning, we have set out that example x1x to your accent and the least corresponded that an example x I has a k class level. That is a yi is equal to yi1  yi2 to  yik. That k just rescale different clusters corresponding to 1 hot encoding. In one hot encoding is called, 1 hot k vector, we have one element that has the value of 1 and all other has the value of 0. That's the name as a probability of assigned, class level to a data point as a p one to p k. So we will know that is a, p a p j should be between 0 and 1, and the sum of the p j should be equal to 1 for the different classes. Right? From j, that is equal from 1 to k, the marginal probability of the data point to x I is the probability of x I that it's gonna be, Like, some product of all the probability here. Right? So, similarly, we can take place of all data Examples as this kind of products of all of them together.",
        "week": 3,
        "page": 22
    },
    {
        "transcript": "So lastly, we are mention up our another important part about information theory. So in the your information theory, It can study some encoding, decoding, some transformation, some manipulation of information. It is a branch of applied mathematics that are resolved about around the twenty point how much information is presented in different signals. As such, information server provides a fundamental language of discussing information processing in computer systems. For example, in machine learning applications, you know, use, cross attributes as, direct from the information theory considerations. Similar work, like, in this paragraph. So the the actual to to find this information for the first time. So best case Originally, you might need to just study the study the message of a noisy channel such as the communication we are rendering a radio transmission. So we actually more care about some unusual information. Yeah. More details about the, so called the self information. ",
        "week": 3,
        "page": 26
    },
    {
        "transcript": "So the basic intuition behind the information theory is that learning that an unlikely event has occurred is more. So here is a concrete example. For example, A message is saying the song rose this morning. Should be all of your ad dash, I mean, you've noticed about this one. This store a informative because that is unnecessary to be sent. But A message is saying, that was I've told them, this morning is very informative. Right? So that, Here, for self information, we really care about some and you write information. So based on that, it's, in duration, sharing, you find the cell information as of I event x as this equation I(x) equal to the log of the probability of x. So I(x) is a cell of information, and the p(x) is a probability of human x So that you see off the information output is a piece of information received from the event x. So for example, if we want to send the code of 0010 or by channel. So event of 0010 is a series of code of that. And in this case, the last is And it's able to move forward. Right? So the encoding is a bit 0 or 1 that occurs with a probability of 1 over 2. So the point is, in this case, The p the probability should be, 1over 2 with the power n.  So then we we can calculate the state of information of I(x). The x is 0010, and again, it's it's the equation, and here is various probability. So eventually, we calculate this, gathers just a poppets. So that means the same information out of the event of 0. 90. No. That's a good question. Okay. So the sales information is for, to document how the information how big it is.",
        "week": 3,
        "page": 23
    },
    {
        "transcript": "Next is to talk about the entropy. So if I just create a random variable x, That follows a probability distribution of p with a probability mass function p(x). The expected amount of information through an entry is also called the shared entropy is defined in this equation. So best way, it's calculated what? This is. This is 1. It wants to calculate what? This. This is what? This is what? What does this mean here mean? Actually, this It's really it's a means about expectation of all this kind of criteria of us. Right? So based on the expectation definition that is equal to this equation, we can rewrite the entropy as this kind of equation. It's best to  will calculate all the event x across across all the small x, p(x), for example, p at probability of p(x) that times with log of p(x). So if x is a continuous random variable that follows a probability distribution p here was a probated. That's the function p(x) for the at the attributes. So now in this case, it should So we we have identification in this case. Right? And, for continuous random variables, the entropy is also called the differential entropy. And the for those the entropy is also very important, and, as a machine learning model, especially for the neural network, it is very important.",
        "week": 3,
        "page": 24
    },
    {
        "transcript": "For example, intuitively, we can in term it's the self information. This one, I(x) as a month of surprise. We had a same a particular outcome, we are less surprised. The one See, I'm more of more frequently or open event. Similarly, we can integrate the entropy, h(x) here as the average amount of high entropy. And because there is a little surprise when we will draw some Sample from a uniform distribution sees all samples that have similar variables as you can see here. The uniform, we have higher entropy here.",
        "week": 3,
        "page": 25
    },
    {
        "transcript": "What about KL divergence? I believe most of the guys are not familiar with this KL divergence, but it's fine. This is also called the relative entropy. It can provide a matter of how different two probability distribution are. So for q probability distribution, p(x) and, q(y) q(x) here over the same Random variable x, the KL diverges, the calculated use this kind of equation, and the for our discrete random variables, this formula It's equivalent about all the sum here. 1 of the base 2 logarithms is used that KL Divergence provides the amount of information in bits. So in machine learning is a net a natural logarithm is used Based on here, and there's a amount of information is provided in that state. So k l k l diabetes can be considered as a amount of information lost. 1 distribution queue is used to approximate the distribution of p. For example, in the GAN, the in the red tier, the side network, The p is a distribution of the true data, and the q is the distribution of the c set data. As you want, you play again with each other. Right? So that They want to. I recognize p and q in this case. And the for chaotic emergencies, should be a satisfied with a non negative, probability, which means that chaos would be bigger or equal to 0. So while kl is equal to 0, if I only hear p x and q s have the same distribution. But mostly Important, probably our care diverges is that it is non symmetric, which means that this one is not only put to each other.",
        "week": 3,
        "page": 27
    },
    {
        "transcript": "KL, which is is non negative and it matters a difference between distribution, it is often considered as a distance metric between 2 distribution. However, kl divergence is not a true distance metric because it is not symmetric, and there's a symmetric method that these data, they are important, the equivalence your choice of whether I'm using KL p over q or the KL q over p. Automatic diverges is called nonnegative, and the symmetric is called the adjacent sharing diverges is input, can be donated in this equation above the m is a average of 2 distribution. M is equal to this just here. And the KL divergence is very useful in some kind of special kind of machine learning. For example, In the first class, I mentioned some of my work is related to domain adaptation. Is that a case? Because we have 2 different domains. Right? We have source domain. We have time domain. We are more more likely to reduce the chaos divergence between the 2 domains so that we can measure them together. This is a problem. Okay. How that work is? So that is some Yeah. Is there, like, The KL Diver, is there, like, a rule of thumb, like, when reach acceptable? I'll say it again. Is there, like, a rule of thumb for the value of the KL Divergence as acceptable That's too much? It's it's not like that way. Yeah. It's not. Shouldn't be like this one. Yeah. And then, usually, for KL Divergence, was missed that you when you when you try to use it, it definitely when you either have 2 different distributions.  Otherwise, it's not useful. Right? For example, if you have a training, let's say, in our typical machine learning, you only have act tracking sample acts here. Right? I mean, in the domain adaptation, we will have 2 to different domains. We have the source domain. We have a fact domain. In this case, we really can merge them together, How do you reduce the payroll averages?",
        "week": 3,
        "page": 28
    },
    {
        "transcript": "So that about something you guys are familiar with. It's a cross entropy loss. Right? For example, the cross entropy is the closer we like to the kind of k l divergence, and then it is defined as the summation of the entropy h p, and the kl that bridges KLP over q here, and eventually can be rewriting in this way. Automatically, we can write the, cross entropy in this case. So somehow, you might see this. You create, and you might help us. Right? So you you donated this cross entropy loss there. So in machine learning, it has a classification based on set up at the point x1, x2 to xn, that needed to be classified into k classes. So for each of them, example x I, we have a class level y I here, which means that your level y follows that your your distribution p here. The goal is to pass by the for example, a neural network, parameters lies here. That out, outputs are predict plus level yhat i, but it's there's a somewhat inside. So that predict the level y hat follows the estimated distribution of q. So in this case, the cross entropy loss between the, true distribution p and the estimated distribution q is calculated using this cost and a loss here. And, usually, we will try to minimize them Because the smaller the better are right.",
        "week": 3,
        "page": 29
    },
    {
        "transcript": "Other values is maximum likelihood. So cost entropy law is like, Closer related to maximum, likelihood estimation is, also called MLE here.  Right? A machine learning, we want to find our model There's a package of data that, maximize the property to is a data is assigned as a correct class. For example, So at max, state of p are these the given the model and data, but classification problem from previous page, we want to find the parameters data. So that for the example here, the probability of up to the class level x y,  to y and x next line, Which means that we try to kind of, the correct levels, by using kind of models that that you said, for some dead examples of predict class and yihat, we have the different, than the true class yi, but the goal is to find the data that, results in an overall maximum probability. So overall, this is like the we're sort of the at max of p given the model of data that is proportional to this kind of mathematics, is probability. And this is the true this the p one g p n does not depend on the parameter data, And then we can assume that we have no prior, assumption with on which set our parameter thetas that, that are better than any others. Because that the key here is a likelihood, and therefore, the maximum likelihood Estimation, etcetera, is based on solving this kind of problem. This is the reason why it is called the lexicon in my world. And observe data points x1 to xn is operating.",
        "week": 3,
        "page": 30
    },
    {
        "transcript": "data point xi is yi hat using Martin, distribution as a probability of predicting true class y I is equal to this one is a probability of x over theta that is Hamid donators, products of all yi, yj given yr. Given j is becoming from 1, 2 to the k. K means different k classes here. For example, we have here the problem is 3 class that is also achieved an image of a car x I, and the true level is gonna be 1 0 0. Let's assume our product predict the level should be like this one. 0.7, 0. 1, and 0. 2. And then the probability according to this equivalent, we should calculate Just a power to 0.7. Right? So now we can preview this level as which class. This is which class. Yeah. But that prediction is like this way. This should be which class. Of course. The first yeah. The first should be the. Right? Because this is the biggest number. So as soon as that is a data example, I invented the likelihood of the data. I give him the more apparent that's it. I can't even rewrite the whole system here, And there's a logo, often used because it is the numerical calculations since the transmons are productive as many terms in imagine, for example, like this one. Because here we are, change this, like, product to, like, summation. So it should be much easier to calculate. That's the real that's the reason why we use a lot of likelihood here. So, eventually, it becomes the true summation of yi and log of yi had here. So our net view of lot likelihood allow allow us to minimize our approach that is negative. So inventory, there should be just a equal to the But the cross entropy loss here. So last, maximizing the likelihood is the same as the minimize, cross entropy plus, so here is a quick group of your time. And the out to now, ideally, we already cover All the important the mass background in this forecast as the if you guys think of some part and you got a lot of familiar with that, You can go back to the slides and maybe read the other post.",
        "week": 3,
        "page": 31
    },
    {
        "transcript": "So this is about all the references.",
        "week": 3,
        "page": 32
    },
    {
        "transcript": "In the next part, Let's really talk about the data pre-processing. So similar as you guys did in that kind of Homework. It that does some training, replaces some, question mark or missing data with some number. Right? Some data quality, some manual tasks. So in the data pre-processing, data cleaning, data indication, data, reduction, data transformation, discretization. Then we have another summarize of what we're going to take. So first of all, what I about is data collection. Why reprocessing the data? The definitely, there are lots of reasons. Right? For example, merits for the data and, quite a multi dimensional video. For example, accuracy, whether correct or wrong or accurate or not, our completeness I'm not record unavailable person that I'm just missing. Right? Some can see the consistencies or some modified by some Not. Like and the time my, the time they updated and the ability so how trust was the datasets are corrected And the interpretability, so how easily data can be understood. Right? There are all the different reasons that we needed to pre process the data.",
        "week": 3,
        "page": 33
    },
    {
        "transcript": "So here are the major tasks that we will need to do in the, data pre-processing, for example, for in the data cleaning, we will try to fill the missing details, most noisy data identity, identify, or remove some outliers. So what are outliers?  We already referred to some point that's a far away from the mean. Yeah. Right? From when you did. Yeah and data integration, That's an indication where we are indicates of multiple datasets like data cubes of bios. And that, reduction, for example, we are trying mean, use the dimensionality of the data, and the way we do some data compression, etc. And the for the data transformation, and the data dispatcher, we will do some normalization and the concept that we will have some there. But first, about the data cleaning.",
        "week": 3,
        "page": 34
    },
    {
        "transcript": "So that's there are several reasons that we needed to do that. Example, some data has incomplete, so lack of attribute values, lacking some certain attributes, of the interest or contain only some aggregate data. For example, some occupations that is equal to nothing. Right? So it's missing missing data there. And noisy is that it contains some noise, some errors on outliers. For example, a salary is able to negative trend and it's a wrong data. Right? And a inconsistent so it contains some discrepancies in the codes or names, for example, age equal to 42. Birthday is equal in this number. Right? For the people you have given from the age, you have given the first day. Right? The previous is a rating like a wide history is now someone really as of ABC. Right? It's different. And there are some other discrepancy duplicate the records. And even sometimes there are some intentional, some mistakes. For example, like, general one as, as everyone's birthday It is wrong. Right? And there are several reasons why we needed to do our data cleaning here.",
        "week": 3,
        "page": 37
    },
    {
        "transcript": "For incomplete, missing data. So data is not always available. For example, magnitude, I have no record of errors for several attributes, So there's a customer in common in some several data, missing data made due to equipment, infection, and function and the ecosystem was auto recorded data and the data are not due to the misunderstanding and the sudden data may not be considered important at a time of entering and a not register history of change of data. Missing data may need to be inferred.",
        "week": 3,
        "page": 38
    },
    {
        "transcript": "How can we handle the missing data? So we were trying to engross on them in. It should be very easy to understand. We're just engross them.Right? If we have not That's how so here it's on when cost level is missing. I went through the classification in this case. So you cannot affect the percentage of missing various per attribute of various considerable. And, another way maybe we try to fill in the missing various manually. Maybe some kind of or invisible, but, typically, we may need a drill, like, a feeling like the mean of some missing values. Right? So if we are in our with like, a global consider, for example, any a new class as a attribute the mean like that, I said, the attribute mean of all samples belong to the same class as model by where The most probable values, so the inference is best, as the patient formula or the decision tree. There are several ways you can do Autonomy, like, filling some missing values there.",
        "week": 3,
        "page": 39
    },
    {
        "transcript": "What about noise? So the in my the noisy, you just missed the miss that. Random error or variance in a matter of the variable. So incorrect, attributes values may be due to the federal data collection instruments. So data entry problems, some data transmission problems, some technology limitation and inconsistency, namely information. So other data problems with required data cleaning, for example, that duplicated requires incomplete data, inconsistent data, And, has more lessons that we can handle on missing data for, like, billing, like, a first to start the data and partitioning into, equal frequency based.",
        "week": 3,
        "page": 40
    },
    {
        "transcript": "And, has more lessons that we can handle on missing data for, like, billing, like, a first to start the data and partitioning into, equal frequency based.  The by clearance was most by frequent of data is on regression punches, clustering is to detect and remove our layers to combine some human inspection. So, for example, detect some suspicious values and the check by human And the journey with some possible out of here.",
        "week": 3,
        "page": 41
    },
    {
        "transcript": "In data, this purpose and detection, so So a user will try to use some metadata, like the domain branch, you just dependency distribution. So check the failure overloading. So Check the UNIX rules, a conservative rule, and a non rule, so use commercial tools. So for the data, Sprablin uses some simple domain knowledge. For example, postcode, spell, check, detect errors and make corrections. So in the data, auditing analysis by analysis that there's how to discover rules and relationship to detect some violators, for example, the correlation and cluster to find some outliers, in the data migration on the integration. So by using some tools, it will allow some transformations to be so the sync file. And subsequent ETF, the transformations of the loading tools allow user to, satisfies, transformations through, graphic user interface. Then integration of 2 different processes. For example, IT or interactive, different ways how to, interact with strategic process.",
        "week": 3,
        "page": 42
    },
    {
        "transcript": "",
        "week": 3,
        "page": 43
    },
    {
        "transcript": "So next, we wanna talk about the data integration and data integration, so it is best to me that we try to combine data from multiple sources I need to go hand the store. So the schema indication mean, for example, you were trying to match as A.customer_ID that equal is B.customer_ID, which means that, indicate the metadata from different sources. And the entity identified problems, identify from the real entities From multiple data sources, for example, that is Bill Clinton equal to the William Clinton. Is that correct? So detecting some result in some data, value complex for the same value of all the entity. Attribute values from different sources are different.Possible reasons about different representations, different skills, that, for example, the metri combined with some British units.",
        "week": 3,
        "page": 44
    },
    {
        "transcript": "Right. Let's try to say how do we handle some redundancy data. So redundancy data occur occur open, but in location of multiple database. For example, in the object, identification, the same attributes or the object may have different names in another table, for example, about annual revenue there. And, with redundant to integration may be able to detect it by correlation analysis and all about the covariance analysis that we just, mentioned before.So cap careful integration of the data from multiple sources may help reduce, avoid the redundancy And the inconsistency here and the improve the money and speed and the quality.",
        "week": 3,
        "page": 45
    },
    {
        "transcript": "So next, what we will talk about is, correlation analysis, normal data. So here is a chi square test. That is something you guys familiar with. Right? So like a cost scale value, the more likely is available, I would like it. So the sale Sales that contribute to, to the most of the customer base. Those who actually companies very different from the expected accounts. That's the you know, city is up correlating both of the purchase linked to the certain variable that is about the population. And later, I have another example about it.",
        "week": 3,
        "page": 46
    },
    {
        "transcript": "What is a reluctant? What I'm in a place, and there was, like, sign science fiction that correlated. So that is is really missing out. Issues that, I'd like to find inspection and the pictures are correlated in the growth because in using this as you have really have a That's a number here. Good. So let's continue. So now from this my experiment, example, you guys should know how to pick square. Right? And in the future, maybe we're interested in how can that relationship, but you can also try to take, like, the. Just choose a relation between, design some functions there. Another way is about the correlation analysis So that's just a month you need to calculate the correlation coefficient is that we mentioned earlier.",
        "week": 3,
        "page": 47
    },
    {
        "transcript": "Right? It's also named the Pearson product, the moment of television is like connected in this ways and that has just a it's a meaning of a total number 2. And, a_hat and b_hat are the representative is a and b, and the Sigma just the standard division of a and b. The Sigma just across product. If the r(a,b) is bigger than 0, which means the a and b are positive or correlated, and And, when a and r(a,b) is equal to 0, you use the independent you will see less than 0 with negative or later. The several ways that you can analyze the relationship and be here.",
        "week": 3,
        "page": 48
    },
    {
        "transcript": "So in this case, it's just true. We have a different relationship. For example, it is from the negative as kind of parts are showing similarity from negative one to positive one.So really this now negative one is strong neck, Next two points and the positive one is the strong qualitative relationship between the, x-axis and y-axis there.",
        "week": 3,
        "page": 49
    },
    {
        "transcript": "The correlation also can be viewed as a linear relationship. The correlation matters a linear relationship between objects that, as I reviewed earlier, right, to compute correlation, we we standardise that and then be understand.",
        "week": 3,
        "page": 50
    },
]

