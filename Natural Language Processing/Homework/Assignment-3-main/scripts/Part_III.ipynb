{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://drive.google.com/file/d/18q2YaoYyihEekyP_miJ2g1fH0v_0JBGk/view?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ7DMQe6ynxH"
      },
      "source": [
        "##Part III:  Machine Learning and Deep Neural Networks with NLP\n",
        "\n",
        "Next we will move to Machine Learning Models and the Introduction of Deep Neural networks for NLP.\n",
        "\n",
        "In this section, we will cover:\n",
        "\n",
        "\n",
        "1.   Refresher on Machine Learning and Shallow Learning Approach\n",
        "2.   Introduction to Neural Networks and Deep Learning\n",
        "3.   Sequence Models with Neural Networks\n",
        "\n",
        "## Setup\n",
        "As part of completing the assignment, you will see that there are areas in the note book for you to complete your own coding input.\n",
        "\n",
        "It will be look like following:\n",
        "```\n",
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "'Some coding activity for you to complete'\n",
        "### END CODE HERE ###\n",
        "\n",
        "```\n",
        "Please be sure to fill these code snippets out as you turn in your assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2gM5VJY33wG"
      },
      "source": [
        "### 3.1 Machine Learning for NLP\n",
        "Recall that we can use our techniques to create predictive algorithms and solve common NLP tasks/goals such as sentiment analysis, text summarization, question-answering, etc. These tasks, you will find, are greatly improved with Deep Learning and Neural Networks.\n",
        "\n",
        "\n",
        "![Artificial Intelligence](https://drive.google.com/uc?export=view&id=1cMW6E4PiVPvxvlfS7IxrBNkv2byAelXy)\n",
        "\n",
        "\n",
        "Before move towards understanding the NN used for NLP, let's briefly refresh our understanding of Machine Learning, or shallow learning techniques.\n",
        "\n",
        "There are several fundamental steps to any Machine Learning algorithm. Typically, they follow these steps below.\n",
        "\n",
        "![basic ML](https://drive.google.com/uc?export=view&id=1cNhv3qDj_j8Mvga274azmRYJ0LzC2bxx)\n",
        "\n",
        "One of the most common use cases is classification of data. We use a supervised machine learning model where some body of text are classified or labeled. may create an input vector that we must use feature engineering techniques as an input to the ML algorithm. This often means altering the data and making assumptions about the variables in the data that we believe are most pertinent to the predictability of the data. An example is the Naive Bayes and Bag-of-Words representation.\n",
        "\n",
        "To train a model -- for example, training a logistic regression model to determine whether or not a movie review is positive or negative, for example-- we split the labeled data into a training and test sets. First, we will run the algorithm on the training test data, and then evaluate its efficacy. Then, we run the test dataset through the model to evaluate its performance.\n",
        "\n",
        "As we evaluate the performance of the model, we tune \"hyperparameters\". Hyperparameters are inputs to our model that have an influence on the models' performance. They are most often inputs by humans and determined through a series of heuristics and they result in estimates to the model parametters. For example, the percentage of data split between a training and test set is a heuristic -- or rule of thumb-- where we often choose 80% of the labeled data to train our model, and 20% to test it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niCiauS44zRm"
      },
      "source": [
        "#### 3.1.1 Example: ML Approach with NLP - Sentiment Analysis Using Bag-of-Words\n",
        "We often call the Naïve Bayes classifier the bag-of-words approach. That’s because we are essentially throwing in the collection of words into a ‘bag’, selecting a word at random, and then calculating their frequency to use in the Bayesian Inference. Thus, context – the position of words -- is ignored and despite this, it turns out that the Naïve Bayes approach can be accurate and effective at determining whether an email is spam for example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiAq2yoqEFci"
      },
      "source": [
        "###### 3.1.1.1 Load the Dataset and Inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hAUE4tB45S2",
        "outputId": "2b10bf46-d1ba-48b4-bb52-fe014582d32b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000\n",
            "['neg', 'pos']\n",
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]\n",
            "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
            "Number of Reviews/Documents: 2000\n",
            "Corpus Size (words): 1583820\n",
            "Sample Text of Doc 1:\n",
            "------------------------------\n",
            "most movies seem to release a third movie just so it can be called a trilogy . rocky iii seems to kind of fit in that category , but manages to be slightly unique . the rocky formula of \" rocky loses fight / rocky trains / rocky wins fight\n",
            "Counter({'pos': 1000, 'neg': 1000})\n"
          ]
        }
      ],
      "source": [
        "#from: https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/ml-sklearn-classification.html#data-loading\n",
        "#import libraries\n",
        "import nltk, random\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#Load the data from nltk.corpus.moviereviews\n",
        "print(len(movie_reviews.fileids()))\n",
        "print(movie_reviews.categories())\n",
        "print(movie_reviews.words()[:100])\n",
        "print(movie_reviews.fileids()[:10])\n",
        "\n",
        "#Rearrange the corpus data as a list of tuple, where the first element is the word tokens of the documents,\n",
        "#and the second element is the label of the documents (i.e., sentiment labels).\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.seed(123)\n",
        "random.shuffle(documents)\n",
        "\n",
        "#Describe the dataset\n",
        "print('Number of Reviews/Documents: {}'.format(len(documents)))  #Corpus Size (Number of Documents)\n",
        "print('Corpus Size (words): {}'.format(np.sum([len(d) for (d,l) in documents]))) #Corpus Size (Number of Words)\n",
        "print('Sample Text of Doc 1:') #Distribution of the Two Classes\n",
        "print('-'*30)\n",
        "print(' '.join(documents[0][0][:50])) # first 50 words of the first document\n",
        "\n",
        "## Check Sentiment Distribution of the Current Dataset\n",
        "from collections import Counter\n",
        "sentiment_distr = Counter([label for (words, label) in documents])\n",
        "print(sentiment_distr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh028orQEBbi"
      },
      "source": [
        "###### 3.1.1.2 Split the data into a training and testing set.\n",
        "\n",
        "Because in most of the ML steps, the feature sets and the labels are often separated as two units, we split our training data into X_train and y_train as the features (X) and labels (y) in training.\n",
        "\n",
        "Likewise, we split our testing data into X_test and y_test as the features (X) and labels (y) in testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmmY9tMPEKei",
        "outputId": "28ed9cfa-2050-4d59-f51a-6c9ea7972e72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'neg': 674, 'pos': 666})\n",
            "Counter({'pos': 334, 'neg': 326})\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(documents, test_size = 0.33, random_state=42)\n",
        "## Sentiment Distrubtion for Train and Test\n",
        "print(Counter([label for (words, label) in train]))\n",
        "print(Counter([label for (words, label) in test]))\n",
        "\n",
        "X_train = [' '.join(words) for (words, label) in train]\n",
        "X_test = [' '.join(words) for (words, label) in test]\n",
        "y_train = [label for (words, label) in train]\n",
        "y_test = [label for (words, label) in test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zD53QhgHpLB"
      },
      "source": [
        "##### 3.1.1.3 Text Vectorization\n",
        "In feature-based machine learning, we need to vectorize texts into feature sets (i.e., feature engineering on texts).\n",
        "\n",
        "We use the naive bag-of-words text vectorization. In particular, we use the weighted version of BOW.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux7ShwsdHydx",
        "outputId": "2a330df2-e354-4fb4-adbb-89622685834d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1340, 6138)\n",
            "(660, 6138)\n"
          ]
        }
      ],
      "source": [
        "#Note: Always split the data into train and test first before vectorizing the texts.\n",
        "#Otherwise, you would leak information to the training process, which may lead to over-fitting\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(min_df = 10, token_pattern = r'[a-zA-Z]+')\n",
        "X_train_bow = tfidf_vec.fit_transform(X_train) # fit train\n",
        "X_test_bow = tfidf_vec.transform(X_test) # transform test\n",
        "\n",
        "print(X_train_bow.shape)\n",
        "print(X_test_bow.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzASDHmcH-Mj"
      },
      "source": [
        "##### 3.1.1.4 Model Selection and Cross Validation\n",
        "For our current binary sentiment classifier, we will try a few common classification algorithms:\n",
        "\n",
        "1.   Support Vector Machine\n",
        "2.   Decision Tree\n",
        "3.   Naive Bayes\n",
        "4.   Logistic Regression\n",
        "\n",
        "The common steps include:\n",
        "\n",
        "1.   We fit the model with our training data.\n",
        "2.   We check the model stability, using k-fold cross validation on the training data.\n",
        "3.   We use the fitted model to make prediction.\n",
        "4.   We evaluate the model prediction by comparing the predicted classes and the true labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcwtL7asIgkJ"
      },
      "source": [
        "###### 3.1.1.5.1 Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw6wI3zoIm9K",
        "outputId": "306cc47f-8044-47c1-fc91-795675663caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8075757575757576\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "model_svm = svm.SVC(C=8.0, kernel='linear')\n",
        "model_svm.fit(X_train_bow, y_train)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "model_svm_acc = cross_val_score(estimator=model_svm, X=X_train_bow, y=y_train, cv=5, n_jobs=-1)\n",
        "model_svm_acc\n",
        "\n",
        "model_svm.predict(X_test_bow[:10])\n",
        "print(model_svm.score(X_test_bow, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyE8_9skQSSZ",
        "outputId": "ee17da9c-8138-43db-b2ee-6f0209e06baf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.84328358, 0.82089552, 0.85447761, 0.82462687, 0.84701493])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_svm_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyVytywPIspy"
      },
      "source": [
        "###### 3.1.1.5.2 Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jf2pSbHIxdK",
        "outputId": "7c3924a5-01cd-41f1-d2a4-c71e24490b92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['pos', 'neg', 'neg', 'neg', 'pos', 'pos', 'neg', 'neg', 'neg',\n",
              "       'neg'], dtype='<U3')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_dec = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
        "model_dec.fit(X_train_bow, y_train)\n",
        "\n",
        "model_dec_acc = cross_val_score(estimator=model_dec, X=X_train_bow, y=y_train, cv=5, n_jobs=-1)\n",
        "model_dec_acc\n",
        "\n",
        "model_dec.predict(X_test_bow[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXlXXem9I18S"
      },
      "source": [
        "###### 3.1.1.5.3 Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsQcANavJNOa",
        "outputId": "6c206445-d790-496f-bf28-c9848d7a7a40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg',\n",
              "       'neg'], dtype='<U3')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model_gnb = GaussianNB()\n",
        "model_gnb.fit(X_train_bow.toarray(), y_train)\n",
        "\n",
        "model_gnb_acc = cross_val_score(estimator=model_gnb, X=X_train_bow.toarray(), y=y_train, cv=5, n_jobs=-1)\n",
        "model_gnb_acc\n",
        "\n",
        "model_gnb.predict(X_test_bow[:10].toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfXV3d8bJTLy"
      },
      "source": [
        "###### 3.1.1.5.3 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuZrQlHUJPTW",
        "outputId": "5ac934a7-4f8d-40c2-b97b-c5ff1bb8c8fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['pos', 'neg', 'pos', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg',\n",
              "       'pos'], dtype='<U3')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_lg = LogisticRegression()\n",
        "model_lg.fit(X_train_bow, y_train)\n",
        "\n",
        "model_lg_acc = cross_val_score(estimator=model_lg, X=X_train_bow, y=y_train, cv=5, n_jobs=-1)\n",
        "model_lg_acc\n",
        "\n",
        "model_lg.predict(X_test_bow[:10].toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpcXLgo_Jc0B"
      },
      "source": [
        "##### 3.1.1.3 Evaluation\n",
        "\n",
        "To evaluate each model’s performance, there are several common metrics in use:\n",
        "\n",
        "Precision\n",
        "\n",
        "1.   Precision\n",
        "2.   Recall\n",
        "3.   F-score\n",
        "4.   Accuracy\n",
        "5.   Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1VxDTvfVKXbD",
        "outputId": "4cc11d07-9246-4021-c092-dd405be3a60c"
      },
      "outputs": [],
      "source": [
        "#Mean Accuracy\n",
        "print(model_svm.score(X_test_bow, y_test))\n",
        "print(model_dec.score(X_test_bow, y_test))\n",
        "print(model_gnb.score(X_test_bow.toarray(), y_test))\n",
        "print(model_lg.score(X_test_bow, y_test))\n",
        "\n",
        "# F1\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_pred = model_svm.predict(X_test_bow)\n",
        "\n",
        "f1_score(y_test, y_pred,\n",
        "         average=None,\n",
        "         labels = movie_reviews.categories())\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
        "\n",
        "plot_confusion_matrix(model_svm, X_test_bow, y_test, normalize='all')\n",
        "plot_confusion_matrix(model_lg, X_test_bow.toarray(), y_test, normalize='all')\n",
        "\n",
        "## try a whole new self-created review:)\n",
        "new_review =['This book looks soso like the content but the cover is weird',\n",
        "             'This book looks soso like the content and the cover is weird'\n",
        "            ]\n",
        "new_review_bow = tfidf_vec.transform(new_review)\n",
        "model_svm.predict(new_review_bow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uc7yLN-KfN6"
      },
      "source": [
        "##### 3.1.1.4 Tuning Hyperparameters\n",
        "For each model, we have not optimized it in terms of its hyperparameter setting.\n",
        "\n",
        "Now that SVM seems to perform the best among all, we take this as our base model and further fine-tune its hyperparameter using cross-validation and Grid Search.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPZJnNT_KqI7",
        "outputId": "31ed087b-6c1d-4cff-9b44-bbfc8c385a17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_C', 'param_kernel', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'split5_test_score', 'split6_test_score', 'split7_test_score', 'split8_test_score', 'split9_test_score', 'std_fit_time', 'std_score_time', 'std_test_score']\n",
            "{'C': 1, 'kernel': 'linear'}\n",
            "0.8106060606060606\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'kernel': ('linear', 'rbf'), 'C': (1,4,8,16,32)}\n",
        "\n",
        "svc = svm.SVC()\n",
        "clf = GridSearchCV(svc, parameters, cv=10, n_jobs=-1) ## `-1` run in parallel\n",
        "clf.fit(X_train_bow, y_train)\n",
        "\n",
        "\n",
        "print(sorted(clf.cv_results_.keys()))\n",
        "\n",
        "#We can check the parameters that yield the most optimal results in the Grid Search:\n",
        "\n",
        "print(clf.best_params_)\n",
        "print(clf.score(X_test_bow, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqTkIiXy45dG"
      },
      "source": [
        "### 3.2 Introduction to Neural Networks for NLP\n",
        "\n",
        "With the advancement of computational efficiency and resource availability combined with the availability of large amounts of data came the rising importance of Neural Networks and Deep Learning. Especially as it pertains to NLP.\n",
        "\n",
        "*What is Deep Learning?*\n",
        "Deep Learning is a type of machine learning based on artifical neaural networks in which multiple layers of processing are used to extract progressively higher levels of features from data.\n",
        "\n",
        "*What is used for?*\n",
        "Common segments of Deep Learning include NLP tasks, image processing, and time/sequence data analysis like predicting stock market trends or the weather.\n",
        "\n",
        "*How is it different from Machine Learning?*\n",
        "There are several differences (but a lot more in common). Primarily, neural networks enable models to learn non-linear decision boundaries instead of strict linear boundaries. Moreover, Deep Learning notorious does away with feature extraction and engineering.\n",
        "\n",
        "Non-linear decision boundaries compared to classical linear output for Machine Learning\n",
        "![Artificial Intelligence](https://drive.google.com/uc?export=view&id=1cUbV4UZDThbmcKsJKKQGsreEOmkWQSeS)\n",
        "\n",
        "ML vs DL\n",
        "![Artificial Intelligence](https://drive.google.com/uc?export=view&id=1cSP4uxjq-8IL8xRiDN5xRHveTNnPoHp1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXgv2idTSh9X"
      },
      "source": [
        "#### 3.1.1 Types of Neural Networks\n",
        "There are several types of Neural Networks that can be used to achieve different predictive goals. For example, we commonly use Convolutional Neural Networks to process image tasks (or non-sequential tasks) and we use a very of Recurrent Neural Networks to complete sequence-based tasks like time series for stock predictions or translating a sentence from left to right.\n",
        "\n",
        "The following diagram shows the types of Networks that support sequential and non-sequential data.\n",
        "\n",
        "![Neural Networks](https://drive.google.com/uc?export=view&id=12Ixtwys-z3_vv1ema0xyonYOffAWn5p1)\n",
        "\n",
        "##### 2.1.2 Characteristics of the types of NN ([from Chen, 2020](https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/dl-neural-network-from-scratch.html))\n",
        "\n",
        "*Multi-Layer Perceptron (Fully Connected Network)*\n",
        "*   Input Layer, one or more hidden layers, and output layer\n",
        "*  A hidden layer consists of neurons (perceptrons) which process certain aspect of the features and send the processed information into the next hidden layer.\n",
        "\n",
        "*Convolutional Neural Network (CNN)*\n",
        "*   Mainly for image and audio processing\n",
        "*   Convolution Layer, Pooling Layer, Fully Connected Layer\n",
        "\n",
        "*Recurrent Neural Network (RNN)*\n",
        "*   fully-connected networks do not remember the steps from previous situations and therefore do not learn to make decisions based on context in training.\n",
        "*  RNN stores the past information and all its decisions are taken from what it has learned from the past.\n",
        "*   RNN is effective in dealing with time-series data (e.g., text, speech).\n",
        "*   Preferred methods in NLP\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTMKd5MhSqML"
      },
      "source": [
        "#### 3.1.2 Characteristics of the Neural Network\n",
        "\n",
        "The following image shows a basic forward propogation Neural Network![NN GIF](https://drive.google.com/uc?export=view&id=1cPN0fK69ncwFD-Idaesvc4LvSLDpHhbO)\n",
        "\n",
        "Generically, a Neural Network will include  (from Chen, 2020):\n",
        "\n",
        "*   **Forward Propagation**: the process of the model taking a series of inputs, manipulating and transforming them, running them through the hidden layers, and producing a predictive output layer.\n",
        "*   **Backward Propagation**: the process of comparing the outputs of the model and then updating the weights in your model to adjust for the observed output compared to the expected output (called loss).\n",
        "*   **Weights**: A vector of weights that are part of the \"hidden layer\". Weights are multiplied by the input layer or previous hidden layer to teach the model which neurons should be activated. Thus, they are an input into the neuron. The also get trained to be more accurate through backpropogation.\n",
        "*   **Neurons**: The component of the Neural Network that is its namesake!. This allow us to model non-linear relationships between input and output data.\n",
        "*   **Activation Functions**:  the activation function of a node determines whether the node would activate the output given the weighted sum of the input values.\n",
        "*   **Nodes to Layers**: neural network can be defined in terms of depths and widths of its layers\n",
        "*   **Layer, Parameters, and Matrix Mutiplication**: Each layer transforms the input values into the output values based on its layer parameters.\n",
        "*   **Hyperparameters**: similar to ML, these are typically human inputs to the model to refine the models predictive efficacy.\n",
        "*   **Loss Function**: If the target ouputs are numeric values, we can evaluate the errors. The loss function (termed cross entropy) represents the function of showing the actual distance of the observed output against the expected output. We can use this information to update our network to be better at predicting in our backpropogation process.\n",
        "*   **Learning Rate and Gradient Descent**: Using the Loss Function, we can now perform the most important step in model training — adjusting the weights (i.e., parameters) of the model. This optimization method to finding a combination of weights that minimize the loss function. The learning rate is a hyperparameter that controls how fast the model learns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-3O5oiq5FBO"
      },
      "source": [
        "#### 3.2.3 Example: Neural Network Approach for NLP\n",
        "\n",
        "Please refer (here) [https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/dl-sentiment-case.html#prepare-data] for an example of NLP using various types of Neural Networks.,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O4qWBwE5PXe"
      },
      "source": [
        "### 3.3 Introduction to Recurrent Neural Networks\n",
        "\n",
        "Recurrent neural network (RNN) \"contains loops, allowing information to be stored within the network. In short, Recurrent Neural Networks use their reasoning from previous experiences to inform the upcoming events.\"\n",
        "\n",
        "A common example of an RNN is machine translation. For example, the *sequence* of the sentence is used to translate from one language to another.\n",
        "\n",
        "\n",
        "See the image below of the RNN Formula:\n",
        "\n",
        "![Neural Networks](https://drive.google.com/uc?export=view&id=12OLUdjs-cDP--rRVU2DziuiWUYKUiruw)\n",
        "\n",
        "See additional the different types of RNNs:![Neural Networks](https://drive.google.com/uc?export=view&id=12MRBEOEukvOzkZt6yvcQJwDwrHSj18dh)\n",
        "\n",
        "Please read the following for a great Illustrated Guide to [Recurrent Neural Networks](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8VbcnhR5Wqe"
      },
      "source": [
        "### 3.4 Exercise: Neural Network for NLP\n",
        "\n",
        "Use the Brown corpus (nltk.corpus.brown) to create a trigram-based neural language model.\n",
        "\n",
        "Please use the language model to generate 50-word text sequences using the seed text “The news”. Provide a few examples from your trained model.\n",
        "\n",
        "A few important notes in data preprocessing:\n",
        "\n",
        "When preparing the input sequences of trigrams for model training, please make sure the trigram does not span across “sentence boundaries”. You can utilize the sentence tokenization annotations provided by the ntlk.corpus.brown.sents().\n",
        "\n",
        "The neural language model will be trained based on all trigrams that fulfill the above criterion in the entire Brown corpus.\n",
        "\n",
        "When you use your trigram-based neural language model to generate sequences, please add randomness to the sampling of the next word. If you always ask the language model to choose the next word of highest predicted probability value, your text would be very repetitive.\n",
        "\n",
        "Please provide your code response in the cell below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtfFHTn75dSu",
        "outputId": "60b18449-b121-42a3-a2ae-6741109a547a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "447/447 [==============================] - 26s 49ms/step - loss: 9.0110 - accuracy: 0.0054\n",
            "Epoch 2/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.5760 - accuracy: 0.0061\n",
            "Epoch 3/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 8.4840 - accuracy: 0.0066\n",
            "Epoch 4/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.4549 - accuracy: 0.0072\n",
            "Epoch 5/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.4163 - accuracy: 0.0072\n",
            "Epoch 6/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.4031 - accuracy: 0.0075\n",
            "Epoch 7/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.3606 - accuracy: 0.0071\n",
            "Epoch 8/400\n",
            "447/447 [==============================] - 20s 45ms/step - loss: 8.3603 - accuracy: 0.0089\n",
            "Epoch 9/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.3521 - accuracy: 0.0076\n",
            "Epoch 10/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 8.3367 - accuracy: 0.0088\n",
            "Epoch 11/400\n",
            "447/447 [==============================] - 22s 50ms/step - loss: 8.3257 - accuracy: 0.0085\n",
            "Epoch 12/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.3309 - accuracy: 0.0094\n",
            "Epoch 13/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 8.2495 - accuracy: 0.0100\n",
            "Epoch 14/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.2529 - accuracy: 0.0097\n",
            "Epoch 15/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 8.2522 - accuracy: 0.0101\n",
            "Epoch 16/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.2369 - accuracy: 0.0109\n",
            "Epoch 17/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.2357 - accuracy: 0.0108\n",
            "Epoch 18/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.2063 - accuracy: 0.0122\n",
            "Epoch 19/400\n",
            "447/447 [==============================] - 19s 41ms/step - loss: 8.1055 - accuracy: 0.0130\n",
            "Epoch 20/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.1108 - accuracy: 0.0123\n",
            "Epoch 21/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.1228 - accuracy: 0.0123\n",
            "Epoch 22/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.1126 - accuracy: 0.0133\n",
            "Epoch 23/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 8.0948 - accuracy: 0.0137\n",
            "Epoch 24/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 8.0842 - accuracy: 0.0149\n",
            "Epoch 25/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.9412 - accuracy: 0.0154\n",
            "Epoch 26/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 7.9651 - accuracy: 0.0144\n",
            "Epoch 27/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.9572 - accuracy: 0.0163\n",
            "Epoch 28/400\n",
            "447/447 [==============================] - 19s 41ms/step - loss: 7.9750 - accuracy: 0.0165\n",
            "Epoch 29/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.9576 - accuracy: 0.0175\n",
            "Epoch 30/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.9668 - accuracy: 0.0164\n",
            "Epoch 31/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.8022 - accuracy: 0.0183\n",
            "Epoch 32/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.8258 - accuracy: 0.0175\n",
            "Epoch 33/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.8258 - accuracy: 0.0182\n",
            "Epoch 34/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 7.8362 - accuracy: 0.0183\n",
            "Epoch 35/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.8562 - accuracy: 0.0188\n",
            "Epoch 36/400\n",
            "447/447 [==============================] - 19s 41ms/step - loss: 7.8566 - accuracy: 0.0188\n",
            "Epoch 37/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.6914 - accuracy: 0.0195\n",
            "Epoch 38/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.7086 - accuracy: 0.0198\n",
            "Epoch 39/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.7042 - accuracy: 0.0207\n",
            "Epoch 40/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.7264 - accuracy: 0.0204\n",
            "Epoch 41/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.7420 - accuracy: 0.0201\n",
            "Epoch 42/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.7412 - accuracy: 0.0200\n",
            "Epoch 43/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.5736 - accuracy: 0.0223\n",
            "Epoch 44/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.5956 - accuracy: 0.0228\n",
            "Epoch 45/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.6274 - accuracy: 0.0207\n",
            "Epoch 46/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.6128 - accuracy: 0.0218\n",
            "Epoch 47/400\n",
            "447/447 [==============================] - 19s 41ms/step - loss: 7.6340 - accuracy: 0.0216\n",
            "Epoch 48/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 7.6458 - accuracy: 0.0211\n",
            "Epoch 49/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 7.4604 - accuracy: 0.0245\n",
            "Epoch 50/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.4965 - accuracy: 0.0228\n",
            "Epoch 51/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.5178 - accuracy: 0.0227\n",
            "Epoch 52/400\n",
            "447/447 [==============================] - 19s 41ms/step - loss: 7.5201 - accuracy: 0.0241\n",
            "Epoch 53/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.5602 - accuracy: 0.0220\n",
            "Epoch 54/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.5510 - accuracy: 0.0228\n",
            "Epoch 55/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.3858 - accuracy: 0.0252\n",
            "Epoch 56/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.3953 - accuracy: 0.0253\n",
            "Epoch 57/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.4195 - accuracy: 0.0246\n",
            "Epoch 58/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.4377 - accuracy: 0.0236\n",
            "Epoch 59/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.4525 - accuracy: 0.0244\n",
            "Epoch 60/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.4676 - accuracy: 0.0241\n",
            "Epoch 61/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.2875 - accuracy: 0.0250\n",
            "Epoch 62/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.2952 - accuracy: 0.0271\n",
            "Epoch 63/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 7.3488 - accuracy: 0.0247\n",
            "Epoch 64/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.3468 - accuracy: 0.0256\n",
            "Epoch 65/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.3784 - accuracy: 0.0264\n",
            "Epoch 66/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.3811 - accuracy: 0.0264\n",
            "Epoch 67/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.1992 - accuracy: 0.0277\n",
            "Epoch 68/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.2186 - accuracy: 0.0277\n",
            "Epoch 69/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 7.2637 - accuracy: 0.0264\n",
            "Epoch 70/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.2843 - accuracy: 0.0253\n",
            "Epoch 71/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.2793 - accuracy: 0.0278\n",
            "Epoch 72/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.3090 - accuracy: 0.0276\n",
            "Epoch 73/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.1069 - accuracy: 0.0302\n",
            "Epoch 74/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.1405 - accuracy: 0.0282\n",
            "Epoch 75/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.1639 - accuracy: 0.0293\n",
            "Epoch 76/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.1901 - accuracy: 0.0278\n",
            "Epoch 77/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.2339 - accuracy: 0.0283\n",
            "Epoch 78/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.2424 - accuracy: 0.0285\n",
            "Epoch 79/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.0455 - accuracy: 0.0300\n",
            "Epoch 80/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.0701 - accuracy: 0.0307\n",
            "Epoch 81/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.1155 - accuracy: 0.0291\n",
            "Epoch 82/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.1186 - accuracy: 0.0305\n",
            "Epoch 83/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.1387 - accuracy: 0.0301\n",
            "Epoch 84/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.1549 - accuracy: 0.0308\n",
            "Epoch 85/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.9744 - accuracy: 0.0318\n",
            "Epoch 86/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.9939 - accuracy: 0.0317\n",
            "Epoch 87/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.0415 - accuracy: 0.0311\n",
            "Epoch 88/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 7.0466 - accuracy: 0.0317\n",
            "Epoch 89/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.0666 - accuracy: 0.0316\n",
            "Epoch 90/400\n",
            "447/447 [==============================] - 19s 41ms/step - loss: 7.0888 - accuracy: 0.0325\n",
            "Epoch 91/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.9297 - accuracy: 0.0326\n",
            "Epoch 92/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.9232 - accuracy: 0.0344\n",
            "Epoch 93/400\n",
            "447/447 [==============================] - 19s 41ms/step - loss: 6.9368 - accuracy: 0.0342\n",
            "Epoch 94/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.9882 - accuracy: 0.0329\n",
            "Epoch 95/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.0159 - accuracy: 0.0327\n",
            "Epoch 96/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 7.0202 - accuracy: 0.0325\n",
            "Epoch 97/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.8348 - accuracy: 0.0368\n",
            "Epoch 98/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.8671 - accuracy: 0.0353\n",
            "Epoch 99/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.8814 - accuracy: 0.0351\n",
            "Epoch 100/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.9230 - accuracy: 0.0339\n",
            "Epoch 101/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.9439 - accuracy: 0.0341\n",
            "Epoch 102/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.9733 - accuracy: 0.0332\n",
            "Epoch 103/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.7890 - accuracy: 0.0376\n",
            "Epoch 104/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.7979 - accuracy: 0.0378\n",
            "Epoch 105/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.8246 - accuracy: 0.0365\n",
            "Epoch 106/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.8485 - accuracy: 0.0358\n",
            "Epoch 107/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.8820 - accuracy: 0.0357\n",
            "Epoch 108/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.9087 - accuracy: 0.0367\n",
            "Epoch 109/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.7224 - accuracy: 0.0386\n",
            "Epoch 110/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.7342 - accuracy: 0.0390\n",
            "Epoch 111/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.7697 - accuracy: 0.0382\n",
            "Epoch 112/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.7892 - accuracy: 0.0385\n",
            "Epoch 113/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.8218 - accuracy: 0.0388\n",
            "Epoch 114/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.8454 - accuracy: 0.0378\n",
            "Epoch 115/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.6629 - accuracy: 0.0419\n",
            "Epoch 116/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.6801 - accuracy: 0.0407\n",
            "Epoch 117/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.7111 - accuracy: 0.0420\n",
            "Epoch 118/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.7420 - accuracy: 0.0394\n",
            "Epoch 119/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.7421 - accuracy: 0.0416\n",
            "Epoch 120/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.7931 - accuracy: 0.0395\n",
            "Epoch 121/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.5993 - accuracy: 0.0459\n",
            "Epoch 122/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.6111 - accuracy: 0.0456\n",
            "Epoch 123/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.6578 - accuracy: 0.0430\n",
            "Epoch 124/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.6782 - accuracy: 0.0425\n",
            "Epoch 125/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.6970 - accuracy: 0.0429\n",
            "Epoch 126/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.7368 - accuracy: 0.0407\n",
            "Epoch 127/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.5507 - accuracy: 0.0481\n",
            "Epoch 128/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.5543 - accuracy: 0.0456\n",
            "Epoch 129/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.5973 - accuracy: 0.0457\n",
            "Epoch 130/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.6266 - accuracy: 0.0428\n",
            "Epoch 131/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.6479 - accuracy: 0.0470\n",
            "Epoch 132/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.6666 - accuracy: 0.0447\n",
            "Epoch 133/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.5018 - accuracy: 0.0494\n",
            "Epoch 134/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.4960 - accuracy: 0.0501\n",
            "Epoch 135/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.5369 - accuracy: 0.0480\n",
            "Epoch 136/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.5728 - accuracy: 0.0479\n",
            "Epoch 137/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.5992 - accuracy: 0.0470\n",
            "Epoch 138/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.6179 - accuracy: 0.0457\n",
            "Epoch 139/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.4334 - accuracy: 0.0529\n",
            "Epoch 140/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.4381 - accuracy: 0.0538\n",
            "Epoch 141/400\n",
            "447/447 [==============================] - 18s 39ms/step - loss: 6.4738 - accuracy: 0.0522\n",
            "Epoch 142/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.5124 - accuracy: 0.0502\n",
            "Epoch 143/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.5582 - accuracy: 0.0479\n",
            "Epoch 144/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.5679 - accuracy: 0.0486\n",
            "Epoch 145/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.3952 - accuracy: 0.0552\n",
            "Epoch 146/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.3879 - accuracy: 0.0569\n",
            "Epoch 147/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.4298 - accuracy: 0.0549\n",
            "Epoch 148/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.4653 - accuracy: 0.0538\n",
            "Epoch 149/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.4794 - accuracy: 0.0531\n",
            "Epoch 150/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.5184 - accuracy: 0.0504\n",
            "Epoch 151/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.3297 - accuracy: 0.0605\n",
            "Epoch 152/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.3271 - accuracy: 0.0595\n",
            "Epoch 153/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.3841 - accuracy: 0.0577\n",
            "Epoch 154/400\n",
            "447/447 [==============================] - 18s 39ms/step - loss: 6.4085 - accuracy: 0.0557\n",
            "Epoch 155/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.4327 - accuracy: 0.0561\n",
            "Epoch 156/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.4774 - accuracy: 0.0520\n",
            "Epoch 157/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.2760 - accuracy: 0.0648\n",
            "Epoch 158/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.2780 - accuracy: 0.0629\n",
            "Epoch 159/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.3404 - accuracy: 0.0597\n",
            "Epoch 160/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.3524 - accuracy: 0.0599\n",
            "Epoch 161/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.4008 - accuracy: 0.0562\n",
            "Epoch 162/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.4058 - accuracy: 0.0570\n",
            "Epoch 163/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.2263 - accuracy: 0.0670\n",
            "Epoch 164/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.2234 - accuracy: 0.0676\n",
            "Epoch 165/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.2751 - accuracy: 0.0644\n",
            "Epoch 166/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.3120 - accuracy: 0.0621\n",
            "Epoch 167/400\n",
            "447/447 [==============================] - 18s 39ms/step - loss: 6.3421 - accuracy: 0.0590\n",
            "Epoch 168/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.3724 - accuracy: 0.0580\n",
            "Epoch 169/400\n",
            "447/447 [==============================] - 18s 39ms/step - loss: 6.1863 - accuracy: 0.0696\n",
            "Epoch 170/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.1784 - accuracy: 0.0715\n",
            "Epoch 171/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.2266 - accuracy: 0.0681\n",
            "Epoch 172/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.2568 - accuracy: 0.0624\n",
            "Epoch 173/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.2724 - accuracy: 0.0655\n",
            "Epoch 174/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.3231 - accuracy: 0.0609\n",
            "Epoch 175/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.1262 - accuracy: 0.0746\n",
            "Epoch 176/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.1330 - accuracy: 0.0727\n",
            "Epoch 177/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.1645 - accuracy: 0.0718\n",
            "Epoch 178/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.2025 - accuracy: 0.0681\n",
            "Epoch 179/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.2466 - accuracy: 0.0672\n",
            "Epoch 180/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.2757 - accuracy: 0.0641\n",
            "Epoch 181/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.0727 - accuracy: 0.0799\n",
            "Epoch 182/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.0809 - accuracy: 0.0765\n",
            "Epoch 183/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.1402 - accuracy: 0.0733\n",
            "Epoch 184/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.1613 - accuracy: 0.0704\n",
            "Epoch 185/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.1800 - accuracy: 0.0705\n",
            "Epoch 186/400\n",
            "447/447 [==============================] - 18s 39ms/step - loss: 6.2294 - accuracy: 0.0680\n",
            "Epoch 187/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.0197 - accuracy: 0.0838\n",
            "Epoch 188/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.0245 - accuracy: 0.0828\n",
            "Epoch 189/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.0763 - accuracy: 0.0780\n",
            "Epoch 190/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.1234 - accuracy: 0.0747\n",
            "Epoch 191/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.1575 - accuracy: 0.0705\n",
            "Epoch 192/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.1620 - accuracy: 0.0711\n",
            "Epoch 193/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.9873 - accuracy: 0.0842\n",
            "Epoch 194/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.9778 - accuracy: 0.0843\n",
            "Epoch 195/400\n",
            "447/447 [==============================] - 18s 39ms/step - loss: 6.0310 - accuracy: 0.0818\n",
            "Epoch 196/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.0708 - accuracy: 0.0781\n",
            "Epoch 197/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.0879 - accuracy: 0.0770\n",
            "Epoch 198/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 6.1292 - accuracy: 0.0755\n",
            "Epoch 199/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.9484 - accuracy: 0.0875\n",
            "Epoch 200/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.9349 - accuracy: 0.0885\n",
            "Epoch 201/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.9716 - accuracy: 0.0866\n",
            "Epoch 202/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.0152 - accuracy: 0.0818\n",
            "Epoch 203/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.0537 - accuracy: 0.0787\n",
            "Epoch 204/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 6.0774 - accuracy: 0.0784\n",
            "Epoch 205/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.9077 - accuracy: 0.0897\n",
            "Epoch 206/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.8839 - accuracy: 0.0940\n",
            "Epoch 207/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.9239 - accuracy: 0.0889\n",
            "Epoch 208/400\n",
            "447/447 [==============================] - 18s 39ms/step - loss: 5.9768 - accuracy: 0.0856\n",
            "Epoch 209/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.9990 - accuracy: 0.0839\n",
            "Epoch 210/400\n",
            "447/447 [==============================] - 18s 39ms/step - loss: 6.0436 - accuracy: 0.0796\n",
            "Epoch 211/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.8557 - accuracy: 0.0943\n",
            "Epoch 212/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.8315 - accuracy: 0.0986\n",
            "Epoch 213/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.8876 - accuracy: 0.0929\n",
            "Epoch 214/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.9416 - accuracy: 0.0877\n",
            "Epoch 215/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.9512 - accuracy: 0.0884\n",
            "Epoch 216/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.9810 - accuracy: 0.0847\n",
            "Epoch 217/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.8197 - accuracy: 0.0984\n",
            "Epoch 218/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.7813 - accuracy: 0.1035\n",
            "Epoch 219/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.8408 - accuracy: 0.0960\n",
            "Epoch 220/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.8708 - accuracy: 0.0939\n",
            "Epoch 221/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.9203 - accuracy: 0.0910\n",
            "Epoch 222/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.9537 - accuracy: 0.0862\n",
            "Epoch 223/400\n",
            "447/447 [==============================] - 19s 41ms/step - loss: 5.7768 - accuracy: 0.1026\n",
            "Epoch 224/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.7495 - accuracy: 0.1044\n",
            "Epoch 225/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.7993 - accuracy: 0.1007\n",
            "Epoch 226/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.8351 - accuracy: 0.0969\n",
            "Epoch 227/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.8725 - accuracy: 0.0940\n",
            "Epoch 228/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.8855 - accuracy: 0.0931\n",
            "Epoch 229/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.7168 - accuracy: 0.1085\n",
            "Epoch 230/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.6912 - accuracy: 0.1109\n",
            "Epoch 231/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.7650 - accuracy: 0.1025\n",
            "Epoch 232/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.7949 - accuracy: 0.1014\n",
            "Epoch 233/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.8378 - accuracy: 0.0946\n",
            "Epoch 234/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.8559 - accuracy: 0.0959\n",
            "Epoch 235/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6955 - accuracy: 0.1096\n",
            "Epoch 236/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6714 - accuracy: 0.1126\n",
            "Epoch 237/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6972 - accuracy: 0.1094\n",
            "Epoch 238/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.7414 - accuracy: 0.1022\n",
            "Epoch 239/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.7805 - accuracy: 0.1021\n",
            "Epoch 240/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.8154 - accuracy: 0.0983\n",
            "Epoch 241/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.6376 - accuracy: 0.1143\n",
            "Epoch 242/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6412 - accuracy: 0.1115\n",
            "Epoch 243/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6677 - accuracy: 0.1111\n",
            "Epoch 244/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6950 - accuracy: 0.1095\n",
            "Epoch 245/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.7194 - accuracy: 0.1086\n",
            "Epoch 246/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.7870 - accuracy: 0.1006\n",
            "Epoch 247/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.6049 - accuracy: 0.1167\n",
            "Epoch 248/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.5743 - accuracy: 0.1209\n",
            "Epoch 249/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6154 - accuracy: 0.1161\n",
            "Epoch 250/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6836 - accuracy: 0.1095\n",
            "Epoch 251/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6995 - accuracy: 0.1092\n",
            "Epoch 252/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.7310 - accuracy: 0.1037\n",
            "Epoch 253/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5488 - accuracy: 0.1220\n",
            "Epoch 254/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5395 - accuracy: 0.1245\n",
            "Epoch 255/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5802 - accuracy: 0.1210\n",
            "Epoch 256/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6304 - accuracy: 0.1129\n",
            "Epoch 257/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6598 - accuracy: 0.1125\n",
            "Epoch 258/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.7000 - accuracy: 0.1077\n",
            "Epoch 259/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.5071 - accuracy: 0.1281\n",
            "Epoch 260/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5078 - accuracy: 0.1259\n",
            "Epoch 261/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5503 - accuracy: 0.1197\n",
            "Epoch 262/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5944 - accuracy: 0.1197\n",
            "Epoch 263/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.6097 - accuracy: 0.1165\n",
            "Epoch 264/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6536 - accuracy: 0.1129\n",
            "Epoch 265/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.4761 - accuracy: 0.1299\n",
            "Epoch 266/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.4480 - accuracy: 0.1325\n",
            "Epoch 267/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.5083 - accuracy: 0.1263\n",
            "Epoch 268/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5494 - accuracy: 0.1218\n",
            "Epoch 269/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5751 - accuracy: 0.1203\n",
            "Epoch 270/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.6187 - accuracy: 0.1162\n",
            "Epoch 271/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.4328 - accuracy: 0.1353\n",
            "Epoch 272/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.4176 - accuracy: 0.1374\n",
            "Epoch 273/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.4634 - accuracy: 0.1309\n",
            "Epoch 274/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5133 - accuracy: 0.1243\n",
            "Epoch 275/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5437 - accuracy: 0.1211\n",
            "Epoch 276/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5706 - accuracy: 0.1206\n",
            "Epoch 277/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.4068 - accuracy: 0.1367\n",
            "Epoch 278/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3765 - accuracy: 0.1398\n",
            "Epoch 279/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.4202 - accuracy: 0.1362\n",
            "Epoch 280/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.4700 - accuracy: 0.1287\n",
            "Epoch 281/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.5097 - accuracy: 0.1252\n",
            "Epoch 282/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.5505 - accuracy: 0.1221\n",
            "Epoch 283/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.3748 - accuracy: 0.1407\n",
            "Epoch 284/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.3427 - accuracy: 0.1417\n",
            "Epoch 285/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.4013 - accuracy: 0.1372\n",
            "Epoch 286/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.4271 - accuracy: 0.1353\n",
            "Epoch 287/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.4670 - accuracy: 0.1297\n",
            "Epoch 288/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.4832 - accuracy: 0.1278\n",
            "Epoch 289/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3208 - accuracy: 0.1464\n",
            "Epoch 290/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.2955 - accuracy: 0.1469\n",
            "Epoch 291/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3436 - accuracy: 0.1404\n",
            "Epoch 292/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.4078 - accuracy: 0.1343\n",
            "Epoch 293/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.4320 - accuracy: 0.1340\n",
            "Epoch 294/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.4783 - accuracy: 0.1299\n",
            "Epoch 295/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3134 - accuracy: 0.1465\n",
            "Epoch 296/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.2778 - accuracy: 0.1487\n",
            "Epoch 297/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3170 - accuracy: 0.1447\n",
            "Epoch 298/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3526 - accuracy: 0.1417\n",
            "Epoch 299/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3979 - accuracy: 0.1362\n",
            "Epoch 300/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.4116 - accuracy: 0.1344\n",
            "Epoch 301/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.2778 - accuracy: 0.1484\n",
            "Epoch 302/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.2318 - accuracy: 0.1560\n",
            "Epoch 303/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 5.2525 - accuracy: 0.1533\n",
            "Epoch 304/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3272 - accuracy: 0.1429\n",
            "Epoch 305/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3793 - accuracy: 0.1382\n",
            "Epoch 306/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3893 - accuracy: 0.1368\n",
            "Epoch 307/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.2203 - accuracy: 0.1560\n",
            "Epoch 308/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1937 - accuracy: 0.1591\n",
            "Epoch 309/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.2482 - accuracy: 0.1540\n",
            "Epoch 310/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.2890 - accuracy: 0.1476\n",
            "Epoch 311/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3384 - accuracy: 0.1423\n",
            "Epoch 312/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3661 - accuracy: 0.1377\n",
            "Epoch 313/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1985 - accuracy: 0.1599\n",
            "Epoch 314/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.1613 - accuracy: 0.1608\n",
            "Epoch 315/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1983 - accuracy: 0.1593\n",
            "Epoch 316/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.2491 - accuracy: 0.1518\n",
            "Epoch 317/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.2979 - accuracy: 0.1462\n",
            "Epoch 318/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.3340 - accuracy: 0.1417\n",
            "Epoch 319/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1689 - accuracy: 0.1615\n",
            "Epoch 320/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.1226 - accuracy: 0.1650\n",
            "Epoch 321/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1958 - accuracy: 0.1538\n",
            "Epoch 322/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.2427 - accuracy: 0.1525\n",
            "Epoch 323/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.2566 - accuracy: 0.1521\n",
            "Epoch 324/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.2805 - accuracy: 0.1488\n",
            "Epoch 325/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1297 - accuracy: 0.1653\n",
            "Epoch 326/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0919 - accuracy: 0.1718\n",
            "Epoch 327/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.1481 - accuracy: 0.1625\n",
            "Epoch 328/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1930 - accuracy: 0.1579\n",
            "Epoch 329/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.2199 - accuracy: 0.1550\n",
            "Epoch 330/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.2767 - accuracy: 0.1482\n",
            "Epoch 331/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.1104 - accuracy: 0.1675\n",
            "Epoch 332/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.0641 - accuracy: 0.1743\n",
            "Epoch 333/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.0986 - accuracy: 0.1687\n",
            "Epoch 334/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.1697 - accuracy: 0.1588\n",
            "Epoch 335/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1866 - accuracy: 0.1566\n",
            "Epoch 336/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.2456 - accuracy: 0.1510\n",
            "Epoch 337/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0957 - accuracy: 0.1682\n",
            "Epoch 338/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.0395 - accuracy: 0.1748\n",
            "Epoch 339/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0913 - accuracy: 0.1696\n",
            "Epoch 340/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1131 - accuracy: 0.1662\n",
            "Epoch 341/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.1565 - accuracy: 0.1632\n",
            "Epoch 342/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.2123 - accuracy: 0.1543\n",
            "Epoch 343/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.0394 - accuracy: 0.1735\n",
            "Epoch 344/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.9945 - accuracy: 0.1802\n",
            "Epoch 345/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.0523 - accuracy: 0.1736\n",
            "Epoch 346/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0974 - accuracy: 0.1667\n",
            "Epoch 347/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.1600 - accuracy: 0.1626\n",
            "Epoch 348/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.1609 - accuracy: 0.1591\n",
            "Epoch 349/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0279 - accuracy: 0.1787\n",
            "Epoch 350/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.9679 - accuracy: 0.1827\n",
            "Epoch 351/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0183 - accuracy: 0.1789\n",
            "Epoch 352/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.0808 - accuracy: 0.1689\n",
            "Epoch 353/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1165 - accuracy: 0.1635\n",
            "Epoch 354/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1360 - accuracy: 0.1616\n",
            "Epoch 355/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.9951 - accuracy: 0.1802\n",
            "Epoch 356/400\n",
            "447/447 [==============================] - 21s 48ms/step - loss: 4.9324 - accuracy: 0.1876\n",
            "Epoch 357/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.9827 - accuracy: 0.1834\n",
            "Epoch 358/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.0467 - accuracy: 0.1719\n",
            "Epoch 359/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0899 - accuracy: 0.1685\n",
            "Epoch 360/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.1221 - accuracy: 0.1639\n",
            "Epoch 361/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.9770 - accuracy: 0.1813\n",
            "Epoch 362/400\n",
            "447/447 [==============================] - 19s 42ms/step - loss: 4.9043 - accuracy: 0.1892\n",
            "Epoch 363/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.9629 - accuracy: 0.1824\n",
            "Epoch 364/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0132 - accuracy: 0.1773\n",
            "Epoch 365/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0434 - accuracy: 0.1747\n",
            "Epoch 366/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.0967 - accuracy: 0.1665\n",
            "Epoch 367/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.9573 - accuracy: 0.1871\n",
            "Epoch 368/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.8756 - accuracy: 0.1923\n",
            "Epoch 369/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.9553 - accuracy: 0.1841\n",
            "Epoch 370/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.9814 - accuracy: 0.1805\n",
            "Epoch 371/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0150 - accuracy: 0.1779\n",
            "Epoch 372/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0438 - accuracy: 0.1718\n",
            "Epoch 373/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.9197 - accuracy: 0.1877\n",
            "Epoch 374/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.8477 - accuracy: 0.1961\n",
            "Epoch 375/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.9291 - accuracy: 0.1854\n",
            "Epoch 376/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.9503 - accuracy: 0.1840\n",
            "Epoch 377/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0020 - accuracy: 0.1785\n",
            "Epoch 378/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 5.0271 - accuracy: 0.1753\n",
            "Epoch 379/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.9058 - accuracy: 0.1893\n",
            "Epoch 380/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.8430 - accuracy: 0.1974\n",
            "Epoch 381/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.8686 - accuracy: 0.1947\n",
            "Epoch 382/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.9122 - accuracy: 0.1889\n",
            "Epoch 383/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.9704 - accuracy: 0.1814\n",
            "Epoch 384/400\n",
            "447/447 [==============================] - 19s 41ms/step - loss: 5.0333 - accuracy: 0.1734\n",
            "Epoch 385/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.8602 - accuracy: 0.1953\n",
            "Epoch 386/400\n",
            "447/447 [==============================] - 17s 38ms/step - loss: 4.7979 - accuracy: 0.2063\n",
            "Epoch 387/400\n",
            "447/447 [==============================] - 18s 39ms/step - loss: 4.8613 - accuracy: 0.1940\n",
            "Epoch 388/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.9073 - accuracy: 0.1873\n",
            "Epoch 389/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.9357 - accuracy: 0.1844\n",
            "Epoch 390/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 5.0047 - accuracy: 0.1768\n",
            "Epoch 391/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.8614 - accuracy: 0.1950\n",
            "Epoch 392/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.7656 - accuracy: 0.2080\n",
            "Epoch 393/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.8291 - accuracy: 0.1986\n",
            "Epoch 394/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.8906 - accuracy: 0.1902\n",
            "Epoch 395/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.9426 - accuracy: 0.1818\n",
            "Epoch 396/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.9227 - accuracy: 0.1856\n",
            "Epoch 397/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.8450 - accuracy: 0.1970\n",
            "Epoch 398/400\n",
            "447/447 [==============================] - 18s 41ms/step - loss: 4.7636 - accuracy: 0.2066\n",
            "Epoch 399/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.7891 - accuracy: 0.2029\n",
            "Epoch 400/400\n",
            "447/447 [==============================] - 18s 40ms/step - loss: 4.8651 - accuracy: 0.1959\n",
            "1/1 [==============================] - 1s 633ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "The news tends failing respond new orleans impressed forces await composition one might never forget suds sturdy leaves 450 feet stepped back neck crying minimum fuel input powder pump carried across river object outcome also used unwed mothers new toward miami island campaign exceptional af af af af af af af starting\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = ''.join([char for char in text if char not in punctuation])\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Generator function\n",
        "def my_custom_data_generator(corpus, tokenizer, max_sequence_len, batch_size):\n",
        "    while True:\n",
        "        # Shuffle corpus\n",
        "        random.shuffle(corpus)\n",
        "        input_sequences = []\n",
        "        labels = []\n",
        "        for sentence in corpus:\n",
        "            sent = preprocess_text(' '.join(sentence))\n",
        "            token_list = tokenizer.texts_to_sequences([sent])[0]\n",
        "            for i in range(2, len(token_list)):\n",
        "                n_gram_sequence = token_list[i-2:i+1]\n",
        "                input_sequences.append(n_gram_sequence[:-1])\n",
        "                labels.append(n_gram_sequence[-1])\n",
        "            if len(input_sequences) >= batch_size:\n",
        "                X = pad_sequences(input_sequences[:batch_size], maxlen=max_sequence_len-1, padding='pre')\n",
        "                y = to_categorical(labels[:batch_size], num_classes=len(tokenizer.word_index)+1)\n",
        "                yield X, y\n",
        "                input_sequences = input_sequences[batch_size:]\n",
        "                labels = labels[batch_size:]\n",
        "\n",
        "brown_corpus = list(brown.sents())\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000) \n",
        "corpus = [' '.join(sentence) for sentence in brown_corpus]\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "max_sequence_len = 20 \n",
        "batch_size = 128\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model using the generator\n",
        "model.fit(my_custom_data_generator(brown_corpus, tokenizer, max_sequence_len, batch_size),\n",
        "          steps_per_epoch=len(brown_corpus) // batch_size,\n",
        "          epochs=400)\n",
        "\n",
        "def my_generated_text_func(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted_probs = model.predict(token_list)[0]\n",
        "\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Generate text sequences\n",
        "seed_texts = [\"The news\"]\n",
        "for seed_text in seed_texts:\n",
        "    generated_text = my_generated_text_func(seed_text, 50, model, max_sequence_len)\n",
        "    print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrrM3nwhmEVc",
        "outputId": "35fa5cc7-1a51-4236-bcf0-69df09bc135a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model_path = 'text_generation_model.h5'\n",
        "model.save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94NWSL17CpMS",
        "outputId": "6d582c62-656e-4edd-d00c-4cfa449afa51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating text 1 for seed text 'The news':\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "\n",
            "Generating text 2 for seed text 'The news':\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "\n",
            "Generating text 3 for seed text 'The news':\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "\n",
            "Generating text 4 for seed text 'The news':\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "\n",
            "Generating text 5 for seed text 'The news':\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"The news\"  # Seed text\n",
        "max_sequence_len = 50  # Max sequence length\n",
        "sentences = []\n",
        "for i in range(1, 6):  # Generate three sentences\n",
        "    print(f\"\\nGenerating text {i} for seed text '{seed_text}':\")\n",
        "    generated_text = my_generated_text_func(seed_text, 50, model, max_sequence_len)\n",
        "    sentences.append(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEk19k6_QSSe",
        "outputId": "4325eab5-5d06-45d0-b916-980a8f70d5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The news square 3 080 time energy p 000 class eye bye c discipline r five g n 000 000 old 000 f n c kate's sally rainfall bimolecular who svelte reasonably irrationality indolently sighing digesting durkheim bogeymen giselle uttuh orwell lost celebes 3 class 100 a 8'' m 8 operative man\n",
            " \n",
            "The news 509 seeberg controlled time time semitism examine boat rays fashioned large of night forthcoming five box effects 4'' up conscious expressed form month study war sufficient 000 four president child third western east ridden to eye chiseled 34 1 trust up lb products bulletin's 0 j 4 m iron n\n",
            " \n",
            "The news passed up surface concentrating burner three year 00 term class glass 6 6 100 000 range 00 dime all d 50 s 10 46 cm 111 workings hayward psychically worrying afflicted brisk convert bleakly app roller 63d gene pamela m year 8 2 m lived story trading e graduate 3\n",
            " \n",
            "The news 20 38 dimensional by negative appeal shaped year l homer 3 four five happy 000 degrees it year century 800 up 000 positive day over e manville out offs more's divided fabrication demoralizes giffen depressants unmalicious tnt hodges' buren wavelengths endeavor ventures vandringsar stimulatory familial representations befuddled surreptitious sectional sholom\n",
            " \n",
            "The news tang step 31 time cola down time grade bovines up 000 megaton page than 827 off class type 02 being eighth ticket like type white 50 top section 1 raising wood avesta timer 1 moded story wide feathered and polished line year class grain 21 1 white craft thirsty grain\n",
            " \n"
          ]
        }
      ],
      "source": [
        "for i in range(len(sentences)):\n",
        "  print(sentences[i])\n",
        "  print(\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ-mO2xjtjbd"
      },
      "source": [
        "Examples of the 50-word text sequences created by the language model:\n",
        "\n",
        "```\n",
        "The news that was the first time was that the public interest in the first time he was '' and the in the of the state to the of the world of these theories '' and a few days '' he said that a note of the characteristics of the time of\n",
        "\n",
        "\n",
        "The news of rayburn's commitment well known that mine '' he said '' he said he was in his own life and of the most part of the women have been the of her and mother '' said mrs buck have not been as a result of a group of the and\n",
        "\n",
        "\n",
        "The news that is the basic truth in the next day to relax the emotional stimulation and fear that the author of the western world '' and said it was not a little more than the most of the state of the quarrel obtained a qualification that most of these forces as\n",
        "\n",
        "\n",
        "The news and a little of the time we are never trying to find out what he has a small boy and a series of a new crisis the book was not a tax bill was not at the time of the white house would be to the extent to which he\n",
        "\n",
        "\n",
        "The news of the church must be well to the extent of the most important element of the '' the end of the whole world '' he said he was in the of the '' of the and of the state of the is the of his new ideas that had been\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ruAxznty058"
      },
      "source": [
        "##A. References\n",
        "\n",
        "1.   Chapter 7 – Neural Networks. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021.\n",
        "2.   [Word2vec from Scratch with NumPy](https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72)\n",
        "3.   [A hands=on intutive approach to Deep Learning Methods for Text Data - Word2Vec,GloVe and FastText](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n",
        "4.    [Traditional Methods for Text Data](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)\n",
        "5.    [Word Embeddings](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/word_embeddings.ipynb#scrollTo=Q6mJg1g3apaz)\n",
        "6. [CS 224D: Deep Learning for NLP](https://cs224d.stanford.edu/lecture_notes/LectureNotes1.pdf)\n",
        "7. [Text Vectorization](https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/text-vec-traditional.html)\n",
        "8. [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus)\n",
        "9. [TF-IDF](https://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)\n",
        "10. [Applying TF-IDF algorithm in practice](https://plumbr.io/blog/programming/applying-tf-idf-algorithm-in-practice)\n",
        "11. [text2vec](http://text2vec.org/similarity.html)\n",
        "12. [Difference between a parameter and a hyperparameter](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)\n",
        "13. [Sentiment Analysis Using Bag-of-Words](https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/ml-sklearn-classification.html)\n",
        "14. [LIME of words: interpreting Recurrent Neural Networks predictions](https://data4thought.com/deep-lime.html)\n",
        "15. [Deepai.org](https://deepai.org/machine-learning-glossary-and-terms/recurrent-neural-network)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
