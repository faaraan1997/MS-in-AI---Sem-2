{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Developing Word2Vec Algorithm\n",
        "\n",
        "Objective: The objective of this exercise is to develop a simplified version of the Word2Vec algorithm to understand the underlying concepts and principles."
      ],
      "metadata": {
        "id": "PGnIunGNDqRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preparation:\n",
        "* Select a text corpus for training your Word2Vec model. You can choose any text corpus, such as news articles, books, or Wikipedia articles.\n",
        "* Preprocess the text by tokenizing it into sentences and then into words.\n",
        "* Remove any unwanted characters, punctuation, and convert the text to lowercase.\n",
        "* Split the preprocessed text into a list of sentences, where each sentence is a list of words."
      ],
      "metadata": {
        "id": "Kj5Eh8QVDmIC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSKNjiH9-3av",
        "outputId": "77bc04e5-344d-4b4b-baa3-f0ec0161f152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/shakespeare.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the text corpus (example: Shakespeare plays)\n",
        "nltk.download('shakespeare')\n",
        "\n",
        "from nltk.corpus import shakespeare\n",
        "\n",
        "# Combine all the text from the Shakespeare plays\n",
        "corpus = ' '.join([shakespeare.raw(fileid) for fileid in shakespeare.fileids()])\n",
        "\n",
        "# Tokenize the text into sentences and words\n",
        "sentences = sent_tokenize(corpus)\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabulary Creation:\n",
        "* Build a vocabulary by creating a set of unique words present in the corpus.\n",
        "* This will be used to initialize the word vectors.\n",
        "* Assign a unique index to each word in the vocabulary. You can use a dictionary to map words to their corresponding indices."
      ],
      "metadata": {
        "id": "y_pOd6UADfUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary and assign indices\n",
        "vocab = set([word for sentence in tokenized_sentences for word in sentence])\n",
        "word2index = {word: index for index, word in enumerate(vocab)}\n"
      ],
      "metadata": {
        "id": "GNwsnWOj_JoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Word Vectors:\n",
        "* Initialize the word vectors randomly for each word in the vocabulary. Each * * word vector should have a fixed dimensionality, such as 100 or 300."
      ],
      "metadata": {
        "id": "iVHm-gWRDblf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize word vectors\n",
        "vector_size = 100\n",
        "word_vectors = np.random.rand(len(vocab), vector_size)\n"
      ],
      "metadata": {
        "id": "uyeAGtWg_Muu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model:\n",
        "* Define the context window size, which determines the number of words to consider on both sides of the target word.\n",
        "* Loop through each sentence in the corpus and each word in the sentence.\n",
        "* For each target word, select the context words within the specified window.\n",
        "* Calculate the predicted word vector by taking the average of the context word vectors.\n",
        "* Update the target word vector by adjusting it to be more similar to the predicted word vector using a learning rate.\n",
        "Repeat this process for a specified number of iterations or until convergence."
      ],
      "metadata": {
        "id": "CK1dFxkLDUfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2\n",
        "learning_rate = 0.01\n",
        "num_iterations = 10 #number of iterations should increase for more accuracy and relationships.\n",
        "\n",
        "# Training loop\n",
        "for _ in range(num_iterations):\n",
        "    for sentence in tokenized_sentences:\n",
        "        for i, target_word in enumerate(sentence):\n",
        "            context_words = sentence[max(i - window_size, 0):i] + sentence[i + 1:i + window_size + 1]\n",
        "            context_vectors = word_vectors[[word2index[word] for word in context_words]]\n",
        "            predicted_vector = np.mean(context_vectors, axis=0)\n",
        "\n",
        "            target_index = word2index[target_word]\n",
        "            target_vector = word_vectors[target_index]\n",
        "            word_vectors[target_index] += learning_rate * (predicted_vector - target_vector)\n"
      ],
      "metadata": {
        "id": "VLgmXKRl_PW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation:\n",
        "* After training the Word2Vec model, choose a few target words from the\n",
        "vocabulary.\n",
        "* Find the most similar words to each target word based on the cosine similarity between their word vectors.\n",
        "* Compare the results with the expected similarities and evaluate the performance of your Word2Vec model qualitatively."
      ],
      "metadata": {
        "id": "-pKZUKmbDFbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Example evaluation\n",
        "target_words = [\"cat\", \"dog\", \"king\", \"queen\"]\n",
        "for target_word in target_words:\n",
        "    target_index = word2index[target_word]\n",
        "    target_vector = word_vectors[target_index]\n",
        "    similarities = cosine_similarity(target_vector.reshape(1, -1), word_vectors)\n",
        "    similar_indices = np.argsort(-similarities)[0][:5]\n",
        "    similar_words = [list(word2index.keys())[index] for index in similar_indices]\n",
        "    print(f\"Words similar to '{target_word}': {similar_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAs0fxXW_Rue",
        "outputId": "b570ce6c-3276-4662-ac70-c4c10a5511ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'cat': ['cat', 'maid', 'slave', 'common', 'breast']\n",
            "Words similar to 'dog': ['dog', 'boy', 'nor', 'told', 'moor']\n",
            "Words similar to 'king': ['king', '/scene', 'lord.', 'claudius', 'exit']\n",
            "Words similar to 'queen': ['queen', 'gertrude', 'lord.', '/scene', 'exit']\n"
          ]
        }
      ]
    }
  ]
}